{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stanislav Borysov [stabo@dtu.dk], DTU Management*\n",
    "# Advanced Business Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2 - Text Analytics - Part 2: Daily News for Stock Market Prediction [FOR LECTURE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Source: https://www.kaggle.com/aaron7sun/stocknews*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two channels of data provided in this dataset:\n",
    "\n",
    "- News data: Historical news headlines from Reddit WorldNews Channel (/r/worldnews). They are ranked by Reddit users' votes, and only the top 25 headlines are considered for a single date. (Range: 2008-06-08 to 2016-07-01)\n",
    "- Stock data: Dow Jones Industrial Average (DJIA) is used to \"prove the concept\". (Range: 2008-08-08 to 2016-07-01)\n",
    "\n",
    "Three data files in .csv format are provided in `stocknews/`:\n",
    "\n",
    "- `RedditNews.csv`: two columns; The first column is the \"date\", and the second column is the \"news headlines\". All news are ranked from top to bottom based on how hot they are. Hence, there are 25 lines for each date.\n",
    "- `DJIA_table.csv`: Downloaded directly from Yahoo Finance: check out the web page for more info.\n",
    "- `Combined_News_DJIA.csv`: Combined dataset (using the other two files) with 27 columns. The first column is \"Date\", the second is \"Label\", and the following ones are news headlines ranging from \"Top1\" to \"Top25\". The \"Label\" column can take only two values: \"1\" when \"DJIA Adj Close\" value rose or stayed as the same and \"0\" when \"DJIA Adj Close\" value decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Classification: Predict \"Label\" from `Combined_News_DJIA.csv` using news headlines.\n",
    "2. Regression: Predict \"DJIA Adj Close\" from `DJIA_table.csv` using news headlines.\n",
    "\n",
    "For evaluation, please use data from 2008-08-08 to 2014-12-31 as a training set, and a test set is then the following two years of data (from 2015-01-02 to 2016-07-01). This is roughly a 80%/20% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"stocknews/Combined_News_DJIA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>...</th>\n",
       "      <th>Top16</th>\n",
       "      <th>Top17</th>\n",
       "      <th>Top18</th>\n",
       "      <th>Top19</th>\n",
       "      <th>Top20</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>2016-06-27</td>\n",
       "      <td>0</td>\n",
       "      <td>Barclays and RBS shares suspended from trading...</td>\n",
       "      <td>Pope says Church should ask forgiveness from g...</td>\n",
       "      <td>Poland 'shocked' by xenophobic abuse of Poles ...</td>\n",
       "      <td>There will be no second referendum, cabinet ag...</td>\n",
       "      <td>Scotland welcome to join EU, Merkel ally says</td>\n",
       "      <td>Sterling dips below Friday's 31-year low amid ...</td>\n",
       "      <td>No negative news about South African President...</td>\n",
       "      <td>Surge in Hate Crimes in the U.K. Following U.K...</td>\n",
       "      <td>...</td>\n",
       "      <td>German lawyers to probe Erdogan over alleged w...</td>\n",
       "      <td>Boris Johnson says the UK will continue to \"in...</td>\n",
       "      <td>Richard Branson is calling on the UK governmen...</td>\n",
       "      <td>Turkey 'sorry for downing Russian jet'</td>\n",
       "      <td>Edward Snowden lawyer vows new push for pardon...</td>\n",
       "      <td>Brexit opinion poll reveals majority don't wan...</td>\n",
       "      <td>Conservative MP Leave Campaigner: \"The leave c...</td>\n",
       "      <td>Economists predict UK recession, further weake...</td>\n",
       "      <td>New EU 'superstate plan by France, Germany: Cr...</td>\n",
       "      <td>Pakistani clerics declare transgender marriage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>1</td>\n",
       "      <td>2,500 Scientists To Australia: If You Want To ...</td>\n",
       "      <td>The personal details of 112,000 French police ...</td>\n",
       "      <td>S&amp;amp;P cuts United Kingdom sovereign credit r...</td>\n",
       "      <td>Huge helium deposit found in Africa</td>\n",
       "      <td>CEO of the South African state broadcaster qui...</td>\n",
       "      <td>Brexit cost investors $2 trillion, the worst o...</td>\n",
       "      <td>Hong Kong democracy activists call for return ...</td>\n",
       "      <td>Brexit: Iceland president says UK can join 'tr...</td>\n",
       "      <td>...</td>\n",
       "      <td>US, Canada and Mexico pledge 50% of power from...</td>\n",
       "      <td>There is increasing evidence that Australia is...</td>\n",
       "      <td>Richard Branson, the founder of Virgin Group, ...</td>\n",
       "      <td>37,000-yr-old skull from Borneo reveals surpri...</td>\n",
       "      <td>Palestinians stone Western Wall worshipers; po...</td>\n",
       "      <td>Jean-Claude Juncker asks Farage: Why are you h...</td>\n",
       "      <td>\"Romanians for Remainians\" offering a new home...</td>\n",
       "      <td>Brexit: Gibraltar in talks with Scotland to st...</td>\n",
       "      <td>8 Suicide Bombers Strike Lebanon</td>\n",
       "      <td>Mexico's security forces routinely use 'sexual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>2016-06-29</td>\n",
       "      <td>1</td>\n",
       "      <td>Explosion At Airport In Istanbul</td>\n",
       "      <td>Yemeni former president: Terrorism is the offs...</td>\n",
       "      <td>UK must accept freedom of movement to access E...</td>\n",
       "      <td>Devastated: scientists too late to captive bre...</td>\n",
       "      <td>British Labor Party leader Jeremy Corbyn loses...</td>\n",
       "      <td>A Muslim Shop in the UK Was Just Firebombed Wh...</td>\n",
       "      <td>Mexican Authorities Sexually Torture Women in ...</td>\n",
       "      <td>UK shares and pound continue to recover</td>\n",
       "      <td>...</td>\n",
       "      <td>Escape Tunnel, Dug by Hand, Is Found at Holoca...</td>\n",
       "      <td>The land under Beijing is sinking by as much a...</td>\n",
       "      <td>Car bomb and Anti-Islamic attack on Mosque in ...</td>\n",
       "      <td>Emaciated lions in Taiz Zoo are trapped in blo...</td>\n",
       "      <td>Rupert Murdoch describes Brexit as 'wonderful'...</td>\n",
       "      <td>More than 40 killed in Yemen suicide attacks</td>\n",
       "      <td>Google Found Disastrous Symantec and Norton Vu...</td>\n",
       "      <td>Extremist violence on the rise in Germany: Dom...</td>\n",
       "      <td>BBC News: Labour MPs pass Corbyn no-confidence...</td>\n",
       "      <td>Tiny New Zealand town with 'too many jobs' lau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>1</td>\n",
       "      <td>Jamaica proposes marijuana dispensers for tour...</td>\n",
       "      <td>Stephen Hawking says pollution and 'stupidity'...</td>\n",
       "      <td>Boris Johnson says he will not run for Tory pa...</td>\n",
       "      <td>Six gay men in Ivory Coast were abused and for...</td>\n",
       "      <td>Switzerland denies citizenship to Muslim immig...</td>\n",
       "      <td>Palestinian terrorist stabs israeli teen girl ...</td>\n",
       "      <td>Puerto Rico will default on $1 billion of debt...</td>\n",
       "      <td>Republic of Ireland fans to be awarded medal f...</td>\n",
       "      <td>...</td>\n",
       "      <td>Googles free wifi at Indian railway stations i...</td>\n",
       "      <td>Mounting evidence suggests 'hobbits' were wipe...</td>\n",
       "      <td>The men who carried out Tuesday's terror attac...</td>\n",
       "      <td>Calls to suspend Saudi Arabia from UN Human Ri...</td>\n",
       "      <td>More Than 100 Nobel Laureates Call Out Greenpe...</td>\n",
       "      <td>British pedophile sentenced to 85 years in US ...</td>\n",
       "      <td>US permitted 1,200 offshore fracks in Gulf of ...</td>\n",
       "      <td>We will be swimming in ridicule - French beach...</td>\n",
       "      <td>UEFA says no minutes of silence for Istanbul v...</td>\n",
       "      <td>Law Enforcement Sources: Gun Used in Paris Ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
       "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
       "      <td>The president of France says if Brexit won, so...</td>\n",
       "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
       "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
       "      <td>Brazil: Huge spike in number of police killing...</td>\n",
       "      <td>Austria's highest court annuls presidential el...</td>\n",
       "      <td>Facebook wins privacy case, can track any Belg...</td>\n",
       "      <td>...</td>\n",
       "      <td>The United States has placed Myanmar, Uzbekist...</td>\n",
       "      <td>S&amp;amp;P revises European Union credit rating t...</td>\n",
       "      <td>India gets $1 billion loan from World Bank for...</td>\n",
       "      <td>U.S. sailors detained by Iran spoke too much u...</td>\n",
       "      <td>Mass fish kill in Vietnam solved as Taiwan ste...</td>\n",
       "      <td>Philippines president Rodrigo Duterte urges pe...</td>\n",
       "      <td>Spain arrests three Pakistanis accused of prom...</td>\n",
       "      <td>Venezuela, where anger over food shortages is ...</td>\n",
       "      <td>A Hindu temple worker has been killed by three...</td>\n",
       "      <td>Ozone layer hole seems to be healing - US &amp;amp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  Label                                               Top1  \\\n",
       "1984  2016-06-27      0  Barclays and RBS shares suspended from trading...   \n",
       "1985  2016-06-28      1  2,500 Scientists To Australia: If You Want To ...   \n",
       "1986  2016-06-29      1                   Explosion At Airport In Istanbul   \n",
       "1987  2016-06-30      1  Jamaica proposes marijuana dispensers for tour...   \n",
       "1988  2016-07-01      1  A 117-year-old woman in Mexico City finally re...   \n",
       "\n",
       "                                                   Top2  \\\n",
       "1984  Pope says Church should ask forgiveness from g...   \n",
       "1985  The personal details of 112,000 French police ...   \n",
       "1986  Yemeni former president: Terrorism is the offs...   \n",
       "1987  Stephen Hawking says pollution and 'stupidity'...   \n",
       "1988   IMF chief backs Athens as permanent Olympic host   \n",
       "\n",
       "                                                   Top3  \\\n",
       "1984  Poland 'shocked' by xenophobic abuse of Poles ...   \n",
       "1985  S&amp;P cuts United Kingdom sovereign credit r...   \n",
       "1986  UK must accept freedom of movement to access E...   \n",
       "1987  Boris Johnson says he will not run for Tory pa...   \n",
       "1988  The president of France says if Brexit won, so...   \n",
       "\n",
       "                                                   Top4  \\\n",
       "1984  There will be no second referendum, cabinet ag...   \n",
       "1985                Huge helium deposit found in Africa   \n",
       "1986  Devastated: scientists too late to captive bre...   \n",
       "1987  Six gay men in Ivory Coast were abused and for...   \n",
       "1988  British Man Who Must Give Police 24 Hours' Not...   \n",
       "\n",
       "                                                   Top5  \\\n",
       "1984      Scotland welcome to join EU, Merkel ally says   \n",
       "1985  CEO of the South African state broadcaster qui...   \n",
       "1986  British Labor Party leader Jeremy Corbyn loses...   \n",
       "1987  Switzerland denies citizenship to Muslim immig...   \n",
       "1988  100+ Nobel laureates urge Greenpeace to stop o...   \n",
       "\n",
       "                                                   Top6  \\\n",
       "1984  Sterling dips below Friday's 31-year low amid ...   \n",
       "1985  Brexit cost investors $2 trillion, the worst o...   \n",
       "1986  A Muslim Shop in the UK Was Just Firebombed Wh...   \n",
       "1987  Palestinian terrorist stabs israeli teen girl ...   \n",
       "1988  Brazil: Huge spike in number of police killing...   \n",
       "\n",
       "                                                   Top7  \\\n",
       "1984  No negative news about South African President...   \n",
       "1985  Hong Kong democracy activists call for return ...   \n",
       "1986  Mexican Authorities Sexually Torture Women in ...   \n",
       "1987  Puerto Rico will default on $1 billion of debt...   \n",
       "1988  Austria's highest court annuls presidential el...   \n",
       "\n",
       "                                                   Top8  ...  \\\n",
       "1984  Surge in Hate Crimes in the U.K. Following U.K...  ...   \n",
       "1985  Brexit: Iceland president says UK can join 'tr...  ...   \n",
       "1986            UK shares and pound continue to recover  ...   \n",
       "1987  Republic of Ireland fans to be awarded medal f...  ...   \n",
       "1988  Facebook wins privacy case, can track any Belg...  ...   \n",
       "\n",
       "                                                  Top16  \\\n",
       "1984  German lawyers to probe Erdogan over alleged w...   \n",
       "1985  US, Canada and Mexico pledge 50% of power from...   \n",
       "1986  Escape Tunnel, Dug by Hand, Is Found at Holoca...   \n",
       "1987  Googles free wifi at Indian railway stations i...   \n",
       "1988  The United States has placed Myanmar, Uzbekist...   \n",
       "\n",
       "                                                  Top17  \\\n",
       "1984  Boris Johnson says the UK will continue to \"in...   \n",
       "1985  There is increasing evidence that Australia is...   \n",
       "1986  The land under Beijing is sinking by as much a...   \n",
       "1987  Mounting evidence suggests 'hobbits' were wipe...   \n",
       "1988  S&amp;P revises European Union credit rating t...   \n",
       "\n",
       "                                                  Top18  \\\n",
       "1984  Richard Branson is calling on the UK governmen...   \n",
       "1985  Richard Branson, the founder of Virgin Group, ...   \n",
       "1986  Car bomb and Anti-Islamic attack on Mosque in ...   \n",
       "1987  The men who carried out Tuesday's terror attac...   \n",
       "1988  India gets $1 billion loan from World Bank for...   \n",
       "\n",
       "                                                  Top19  \\\n",
       "1984             Turkey 'sorry for downing Russian jet'   \n",
       "1985  37,000-yr-old skull from Borneo reveals surpri...   \n",
       "1986  Emaciated lions in Taiz Zoo are trapped in blo...   \n",
       "1987  Calls to suspend Saudi Arabia from UN Human Ri...   \n",
       "1988  U.S. sailors detained by Iran spoke too much u...   \n",
       "\n",
       "                                                  Top20  \\\n",
       "1984  Edward Snowden lawyer vows new push for pardon...   \n",
       "1985  Palestinians stone Western Wall worshipers; po...   \n",
       "1986  Rupert Murdoch describes Brexit as 'wonderful'...   \n",
       "1987  More Than 100 Nobel Laureates Call Out Greenpe...   \n",
       "1988  Mass fish kill in Vietnam solved as Taiwan ste...   \n",
       "\n",
       "                                                  Top21  \\\n",
       "1984  Brexit opinion poll reveals majority don't wan...   \n",
       "1985  Jean-Claude Juncker asks Farage: Why are you h...   \n",
       "1986       More than 40 killed in Yemen suicide attacks   \n",
       "1987  British pedophile sentenced to 85 years in US ...   \n",
       "1988  Philippines president Rodrigo Duterte urges pe...   \n",
       "\n",
       "                                                  Top22  \\\n",
       "1984  Conservative MP Leave Campaigner: \"The leave c...   \n",
       "1985  \"Romanians for Remainians\" offering a new home...   \n",
       "1986  Google Found Disastrous Symantec and Norton Vu...   \n",
       "1987  US permitted 1,200 offshore fracks in Gulf of ...   \n",
       "1988  Spain arrests three Pakistanis accused of prom...   \n",
       "\n",
       "                                                  Top23  \\\n",
       "1984  Economists predict UK recession, further weake...   \n",
       "1985  Brexit: Gibraltar in talks with Scotland to st...   \n",
       "1986  Extremist violence on the rise in Germany: Dom...   \n",
       "1987  We will be swimming in ridicule - French beach...   \n",
       "1988  Venezuela, where anger over food shortages is ...   \n",
       "\n",
       "                                                  Top24  \\\n",
       "1984  New EU 'superstate plan by France, Germany: Cr...   \n",
       "1985                   8 Suicide Bombers Strike Lebanon   \n",
       "1986  BBC News: Labour MPs pass Corbyn no-confidence...   \n",
       "1987  UEFA says no minutes of silence for Istanbul v...   \n",
       "1988  A Hindu temple worker has been killed by three...   \n",
       "\n",
       "                                                  Top25  \n",
       "1984  Pakistani clerics declare transgender marriage...  \n",
       "1985  Mexico's security forces routinely use 'sexual...  \n",
       "1986  Tiny New Zealand town with 'too many jobs' lau...  \n",
       "1987  Law Enforcement Sources: Gun Used in Paris Ter...  \n",
       "1988  Ozone layer hole seems to be healing - US &amp...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 1984#1983"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bjark\\Anaconda3\\envs\\BjarkiLord\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Barclays and RBS shares suspended from trading after tanking more than 8%.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['Top1'].iloc[loc] = data_df['Top1'].iloc[loc] + \".\"\n",
    "data_df.iloc[loc]['Top1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date                                            2016-06-27\n",
      "Label                                                    0\n",
      "Top1     Barclays and RBS shares suspended from trading...\n",
      "Top2     Pope says Church should ask forgiveness from g...\n",
      "Top3     Poland 'shocked' by xenophobic abuse of Poles ...\n",
      "Top4     There will be no second referendum, cabinet ag...\n",
      "Top5         Scotland welcome to join EU, Merkel ally says\n",
      "Top6     Sterling dips below Friday's 31-year low amid ...\n",
      "Top7     No negative news about South African President...\n",
      "Top8     Surge in Hate Crimes in the U.K. Following U.K...\n",
      "Top9     Weapons shipped into Jordan by the CIA and Sau...\n",
      "Top10    Angela Merkel said the U.K. must file exit pap...\n",
      "Top11    In a birth offering hope to a threatened speci...\n",
      "Top12    Sky News Journalist Left Speechless As Leave M...\n",
      "Top13            Giant panda in Macau gives birth to twins\n",
      "Top14    Get out now: EU leader tells Britain it must i...\n",
      "Top15    Sea turtle 'beaten and left for dead' on beach...\n",
      "Top16    German lawyers to probe Erdogan over alleged w...\n",
      "Top17    Boris Johnson says the UK will continue to \"in...\n",
      "Top18    Richard Branson is calling on the UK governmen...\n",
      "Top19               Turkey 'sorry for downing Russian jet'\n",
      "Top20    Edward Snowden lawyer vows new push for pardon...\n",
      "Top21    Brexit opinion poll reveals majority don't wan...\n",
      "Top22    Conservative MP Leave Campaigner: \"The leave c...\n",
      "Top23    Economists predict UK recession, further weake...\n",
      "Top24    New EU 'superstate plan by France, Germany: Cr...\n",
      "Top25    Pakistani clerics declare transgender marriage...\n",
      "Name: 1984, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data_df.iloc[loc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pope says Church should ask forgiveness from gays for past treatment'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.iloc[loc]['Top2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Economists predict UK recession, further weakening of Pound following Brexit.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.iloc[loc]['Top23']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['text'] = data_df[['Top{}'.format(i) for i in range(1, 26)]].apply(lambda vals: '. '.join([x for x in vals if x is not np.nan]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barclays and RBS shares suspended from trading after tanking more than 8%.. Pope says Church should ask forgiveness from gays for past treatment. Poland 'shocked' by xenophobic abuse of Poles in UK. There will be no second referendum, cabinet agrees. Scotland welcome to join EU, Merkel ally says. Sterling dips below Friday's 31-year low amid Brexit uncertainty. No negative news about South African President allowed on state broadcaster.. Surge in Hate Crimes in the U.K. Following U.K.s Brexit Vote. Weapons shipped into Jordan by the CIA and Saudi Arabia intended for Syrian rebels have been systematically stolen by Jordanian intelligence operatives and sold to arms merchants on the black market, according to American and Jordanian officials. Angela Merkel said the U.K. must file exit papers with the European Union before talks can begin. In a birth offering hope to a threatened species, an aquarium in Osaka, Japan, has succeeded in artificially breeding a southern rockhopper penguin for the first time in the world.. Sky News Journalist Left Speechless As Leave MP Tells Him 'There Is No Plan'. Giant panda in Macau gives birth to twins. Get out now: EU leader tells Britain it must invoke Article 50 on Tuesday. Sea turtle 'beaten and left for dead' on beach by people taking selfies: Loggerhead sea turtle receiving treatment after it was beaten with sticks and stepped on in Lebanon. German lawyers to probe Erdogan over alleged war crimes. Boris Johnson says the UK will continue to \"intensify\" cooperation with the EU and tells his fellow Leave supporters they must accept the 52-48 referendum win was \"not entirely overwhelming\".. Richard Branson is calling on the UK government to hold a second EU referendum to prevent 'irreversible damage' to the country.. Turkey 'sorry for downing Russian jet'. Edward Snowden lawyer vows new push for pardon from Obama. Brexit opinion poll reveals majority don't want second EU referendum: \"half (48%) of British adults say that they are happy with the result, with two in five (43%) saying they are unhappy with the outcome.\". Conservative MP Leave Campaigner: \"The leave campaign don't have a post-Brexit plan...\". Economists predict UK recession, further weakening of Pound following Brexit.. New EU 'superstate plan by France, Germany: Creating a European superstate limiting the powers of individual members following Britains referendum decision to leave the EU. Pakistani clerics declare transgender marriages legal under Islamic law\n",
      "2016-06-27\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(data_df.iloc[loc]['text'])\n",
    "print(data_df.iloc[loc]['Date'])\n",
    "print(data_df.iloc[loc]['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in c:\\users\\bjark\\appdata\\roaming\\python\\python37\\site-packages (3.5)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied, skipping upgrade: joblib in c:\\users\\bjark\\anaconda3\\envs\\bjarkilord\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied, skipping upgrade: click in c:\\users\\bjark\\anaconda3\\envs\\bjarkilord\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in c:\\users\\bjark\\anaconda3\\envs\\bjarkilord\\lib\\site-packages (from nltk) (4.58.0)\n",
      "Requirement already satisfied, skipping upgrade: regex in c:\\users\\bjark\\anaconda3\\envs\\bjarkilord\\lib\\site-packages (from nltk) (2020.11.13)\n"
     ]
    }
   ],
   "source": [
    "pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\bjark/nltk_data'\n    - 'C:\\\\Users\\\\bjark\\\\Anaconda3\\\\envs\\\\BjarkiLord\\\\nltk_data'\n    - 'C:\\\\Users\\\\bjark\\\\Anaconda3\\\\envs\\\\BjarkiLord\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\bjark\\\\Anaconda3\\\\envs\\\\BjarkiLord\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\bjark\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-421d6d1f7736>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgram1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Top2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgram1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\BjarkiLord\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \"\"\"\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     return [\n\u001b[0;32m    146\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\BjarkiLord\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\BjarkiLord\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\BjarkiLord\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\BjarkiLord\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\bjark/nltk_data'\n    - 'C:\\\\Users\\\\bjark\\\\Anaconda3\\\\envs\\\\BjarkiLord\\\\nltk_data'\n    - 'C:\\\\Users\\\\bjark\\\\Anaconda3\\\\envs\\\\BjarkiLord\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\bjark\\\\Anaconda3\\\\envs\\\\BjarkiLord\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\bjark\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "gram1 = word_tokenize(data_df.iloc[loc]['Top2'])\n",
    "print(gram1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\bjark/nltk_data'\n    - 'C:\\\\Users\\\\bjark\\\\Anaconda3\\\\envs\\\\BjarkiLord\\\\nltk_data'\n    - 'C:\\\\Users\\\\bjark\\\\Anaconda3\\\\envs\\\\BjarkiLord\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\bjark\\\\Anaconda3\\\\envs\\\\BjarkiLord\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\bjark\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-bec076912777>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxxx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Giant panda twins born at a zoo in European Union\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxxx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\BjarkiLord\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \"\"\"\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     return [\n\u001b[0;32m    146\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\BjarkiLord\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\BjarkiLord\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\BjarkiLord\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\BjarkiLord\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\bjark/nltk_data'\n    - 'C:\\\\Users\\\\bjark\\\\Anaconda3\\\\envs\\\\BjarkiLord\\\\nltk_data'\n    - 'C:\\\\Users\\\\bjark\\\\Anaconda3\\\\envs\\\\BjarkiLord\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\bjark\\\\Anaconda3\\\\envs\\\\BjarkiLord\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\bjark\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "xxx = word_tokenize(\"Giant panda twins born at a zoo in European Union\")\n",
    "print(xxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pope', 'says'), ('says', 'Church'), ('Church', 'should'), ('should', 'ask'), ('ask', 'forgiveness'), ('forgiveness', 'from'), ('from', 'gays'), ('gays', 'for'), ('for', 'past'), ('past', 'treatment')]\n"
     ]
    }
   ],
   "source": [
    "gram2 = ngrams(gram1, 2)\n",
    "print([x for x in gram2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pope', 'says', 'Church'), ('says', 'Church', 'should'), ('Church', 'should', 'ask'), ('should', 'ask', 'forgiveness'), ('ask', 'forgiveness', 'from'), ('forgiveness', 'from', 'gays'), ('from', 'gays', 'for'), ('gays', 'for', 'past'), ('for', 'past', 'treatment')]\n"
     ]
    }
   ],
   "source": [
    "gram3 = ngrams(gram1, 3)\n",
    "print([x for x in gram3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_transform(corpora):\n",
    "    vocabulary = {}\n",
    "    for d in corpora:\n",
    "        for w in d:\n",
    "            if w not in vocabulary:\n",
    "                vocabulary[w] = len(vocabulary)\n",
    "    voc_len = len(vocabulary)\n",
    "    corpora_bow = []\n",
    "    for d in corpora:\n",
    "        d_bow = [0 for i in range(voc_len)]\n",
    "        for w in d:\n",
    "            d_bow[vocabulary[w]] += 1\n",
    "        corpora_bow.append(d_bow)\n",
    "    return vocabulary, corpora_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'David': 0, 'Cameron': 1, 'to': 2, 'Resign': 3, 'as': 4, 'PM': 5, 'After': 6, 'EU': 7, 'Referendum': 8, '.': 9, 'Prime': 10, 'Minister': 11, 'Resigns': 12, 'European': 13, 'Union': 14, 'referendum': 15, 'Giant': 16, 'panda': 17, 'twins': 18, 'born': 19, 'at': 20, 'a': 21, 'zoo': 22, 'in': 23} [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0]]\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "corpora = [\n",
    "    ['David', 'Cameron', 'to', 'Resign', 'as', 'PM', 'After', 'EU', 'Referendum', '.'],\n",
    "    ['Prime', 'Minister', 'Resigns', 'After', 'European', 'Union', 'referendum', '.'],\n",
    "    ['Giant', 'panda', 'twins', 'born', 'at', 'a', 'zoo', 'in', 'European', 'Union', '.'],\n",
    "    #\n",
    "    [\"Union\", \"After\", \"panda\", \".\"]\n",
    "]\n",
    "vocabulary, corpora_bow = bow_transform(corpora)\n",
    "print(vocabulary, corpora_bow)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['David',\n",
       " 'Cameron',\n",
       " 'to',\n",
       " 'Resign',\n",
       " 'as',\n",
       " 'PM',\n",
       " 'After',\n",
       " 'EU',\n",
       " 'Referendum',\n",
       " '.',\n",
       " 'Prime',\n",
       " 'Minister',\n",
       " 'Resigns',\n",
       " 'European',\n",
       " 'Union',\n",
       " 'referendum',\n",
       " 'Giant',\n",
       " 'panda',\n",
       " 'twins',\n",
       " 'born',\n",
       " 'at',\n",
       " 'a',\n",
       " 'zoo',\n",
       " 'in']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_s = sorted(vocabulary.items(), key=lambda kv: kv[1])\n",
    "[x[0] for x in v_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['David Cameron to Resign as PM After EU Referendum .',\n",
       " 'Prime Minister Resigns After European Union referendum .',\n",
       " 'Giant panda twins born at a zoo in European Union .',\n",
       " 'Union After panda .']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\" \".join(d) for d in corpora]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for d in corpora_bow:\n",
    "    print(np.dot(corpora_bow[-1], d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3162277660168379\n",
      "0.5303300858899106\n",
      "0.4522670168666454\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for d in corpora_bow:\n",
    "    print(1 - cosine(corpora_bow[-1], d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_transformer = TfidfTransformer()\n",
    "corpora_bow_tfidf = tf_transformer.fit_transform(corpora_bow[:-1])\n",
    "q = tf_transformer.transform([corpora_bow[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1960512255643465\n",
      "0.38562352543990863\n",
      "0.3948974401446338\n"
     ]
    }
   ],
   "source": [
    "for d in corpora_bow_tfidf:\n",
    "    print(1 - cosine(q[0].todense(), d.todense()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv(\"stocknews/RedditNews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>The president of France says if Brexit won, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                               News\n",
       "0  2016-07-01  A 117-year-old woman in Mexico City finally re...\n",
       "1  2016-07-01   IMF chief backs Athens as permanent Olympic host\n",
       "2  2016-07-01  The president of France says if Brexit won, so...\n",
       "3  2016-07-01  British Man Who Must Give Police 24 Hours' Not...\n",
       "4  2016-07-01  100+ Nobel laureates urge Greenpeace to stop o..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = news_df['News'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from stopwords import get_stopwords\n",
    "import re, string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(text):\n",
    "    # remove punctuation \n",
    "    text = \"\".join([c for c in text \n",
    "                    if c not in string.punctuation])\n",
    "    # lowercase\n",
    "    text = \"\".join([c.lower() for c in text])\n",
    "    # remove stopwords\n",
    "    text = \" \".join([w for w in text.split() \n",
    "                     if w not in get_stopwords(\"en\")])\n",
    "    # replace numbers with a token\n",
    "    text = re.sub('[0-9]+', 'number', text).lower()\n",
    "    # stemming / lematizing (optional)\n",
    "    text = \" \".join([lemmatizer.lemmatize(w) for w in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processing_vec = np.vectorize(text_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_processing_vec(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "docs = [word_tokenize(d) for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['numberyearold',\n",
       "  'woman',\n",
       "  'mexico',\n",
       "  'city',\n",
       "  'finally',\n",
       "  'received',\n",
       "  'birth',\n",
       "  'certificate',\n",
       "  'died',\n",
       "  'hour',\n",
       "  'later',\n",
       "  'trinidad',\n",
       "  'alvarez',\n",
       "  'lira',\n",
       "  'waited',\n",
       "  'year',\n",
       "  'proof',\n",
       "  'born',\n",
       "  'number'],\n",
       " ['imf', 'chief', 'back', 'athens', 'permanent', 'olympic', 'host'],\n",
       " ['president', 'france', 'say', 'brexit', 'won', 'can', 'donald', 'trump']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:3]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "count_vect = CountVectorizer()\n",
    "bow_counts = count_vect.fit_transform(docs)\n",
    "print(bow_counts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-0b009fd6379b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(docs)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(d) for d in docs]\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.lsimodel import LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_lsi = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = LsiModel(corpus=corpus, id2word=dictionary, num_topics=num_topics_lsi, onepass=False, power_iters=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi[corpus][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi.print_topics(num_topics_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_lda = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics_lda, update_every=1, passes=5, alpha='symmetric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.get_document_topics(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.print_topics(num_topics_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_ids_to_check = [\n",
    "    2814, # 2016-03-10,Gas pipeline from Israel to Jordan to begin operating in 2017\n",
    "    2835, # 2016-03-09,China signals interest in denuclearization talks without North Korea\n",
    "    4034, # 2016-01-21,Chinese president declares support for Palestinian state\n",
    "    11465, # 2015-03-30,Former Israeli Prime Minister Ehud Olmert found guilty in retrial on corruption charges\n",
    "    136, # 2016-06-26,The United Nations issued a strongly worded statement on Friday warning that any attempt by the European Union to bypass national parliaments to push through controversial trade deals would violate international human rights norms and standards.\n",
    "    2242, # 2016-04-03,Bahraini citizens protest for human rights ahead of Formula One race\n",
    "    13901, # 2014-12-22,\"Argentine Court rules: Orang Utans are \"\"non-human-persons\"\" with human rights and therefore need to be released from zoo\"\n",
    "    14708, # 2014-11-20,North Korea has threatened to conduct a nuclear test in response to a United Nations move towards a probe into the country's human rights violations\n",
    "    1732, # 2016-04-23,North Korea seen to fire submarine-launched ballistic missile: South Korea\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['News'].values[14708]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for news_id in news_ids_to_check:\n",
    "    print(80*'-')\n",
    "    print(news_df['News'].values[news_id])\n",
    "    print(docs[news_id])\n",
    "    print(sorted(lsi[corpus[news_id]], key=lambda x: abs(x[1]), reverse=True))\n",
    "    print(sorted(lda[corpus[news_id]], key=lambda x: abs(x[1]), reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGroupedColorFunc(object):\n",
    "    \"\"\"Create a color function object which assigns EXACT colors\n",
    "       to certain words based on the color to words mapping\n",
    "\n",
    "       Parameters\n",
    "       ----------\n",
    "       color_to_words : dict(str -> list(str))\n",
    "         A dictionary that maps a color to the list of words.\n",
    "\n",
    "       default_color : str\n",
    "         Color that will be assigned to a word that's not a member\n",
    "         of any value from color_to_words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, color_to_words, default_color):\n",
    "        self.word_to_color = {word: color\n",
    "                              for (color, words) in color_to_words.items()\n",
    "                              for word in words}\n",
    "\n",
    "        self.default_color = default_color\n",
    "\n",
    "    def __call__(self, word, **kwargs):\n",
    "        return self.word_to_color.get(word, self.default_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_topic(topic, title, color_func):\n",
    "    fig_wordcloud = wordcloud.WordCloud(\n",
    "        max_font_size=100, max_words=50, background_color=\"white\", prefer_horizontal=1, relative_scaling=0.8,\n",
    "    ).fit_words(topic)\n",
    "    if color_func is not None:\n",
    "        fig_wordcloud.recolor(color_func=color_func)\n",
    "    plt.figure(figsize=(10,7), frameon=True)\n",
    "    plt.imshow(fig_wordcloud, interpolation=\"bilinear\")  \n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_negative = {}\n",
    "for i in range(num_topics_lsi):\n",
    "    topic = lsi.show_topic(i, topn=1000)\n",
    "    lsi_negative_topic = []\n",
    "    for w, f in topic:\n",
    "        if f < 0:\n",
    "            lsi_negative_topic.append(w)\n",
    "    lsi_negative[i] = lsi_negative_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_color = '#00AA00'\n",
    "for i in range(num_topics_lsi):\n",
    "    color_to_words = {\n",
    "        # will be colored with a red single color function\n",
    "        'red': lsi_negative[i]\n",
    "    }\n",
    "    grouped_color_func = SimpleGroupedColorFunc(color_to_words, default_color)\n",
    "    viz_topic(dict(map(lambda x: (x[0], abs(x[1])), lsi.show_topic(i, topn=100))), \"Topic #{}\".format(i), grouped_color_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_topics_lda):\n",
    "    viz_topic(dict(lda.show_topic(i, topn=100)), \"Topic #{}\".format(i), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerocessing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(data_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(news_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dates = news_df['Date']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(set(dates))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "labels = []\n",
    "miss = 0\n",
    "for d in dates:\n",
    "    l = data_df[data_df['Date']==d]['Label']\n",
    "    if len(l):\n",
    "        labels.append(int(l))\n",
    "    else:\n",
    "        #print(d)\n",
    "        labels.append(-1)\n",
    "        miss += 1\n",
    "        #break\n",
    "print(miss)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from gensim.matutils import corpus2dense\n",
    "from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tfidf_model = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(tfidf_model[corpus[0]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "doc_n_f = len(labels) - miss\n",
    "dict_n = len(dictionary)\n",
    "#\n",
    "labels_f = np.zeros(doc_n_f)\n",
    "corpus_tf_f = np.zeros((doc_n_f, dict_n))\n",
    "corpus_tfidf_f = np.zeros((doc_n_f, dict_n))\n",
    "corpus_lsi_f = np.zeros((doc_n_f, num_topics_lsi))\n",
    "corpus_lda_f = np.zeros((doc_n_f, num_topics_lda))\n",
    "#\n",
    "print(labels_f.shape)\n",
    "print(corpus_tf_f.shape)\n",
    "print(corpus_tfidf_f.shape)\n",
    "print(corpus_lsi_f.shape)\n",
    "print(corpus_lda_f.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "c = 0\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] != -1:\n",
    "        labels_f[c] = labels[i]\n",
    "        c += 1\n",
    "c"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "c = 0\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] != -1:\n",
    "        labels_f[c] = labels[i]\n",
    "        #\n",
    "        for t, f in corpus[i]:\n",
    "            corpus_tf_f[c, t] = f\n",
    "        #\n",
    "        tfidf_doc = tfidf_model[corpus[i]]\n",
    "        for t, f in tfidf_doc:\n",
    "            corpus_tfidf_f[c, t] = f\n",
    "        #\n",
    "        lsi_doc = lsi[corpus[i]]\n",
    "        for t, f in lsi_doc:\n",
    "            corpus_lsi_f[c, t] = f\n",
    "        #\n",
    "        lda_doc = lda[corpus[i]]\n",
    "        for t, f in lda_doc:\n",
    "            corpus_lda_f[c, t] = f\n",
    "        \"\"\"\n",
    "        print(labels_f[i])\n",
    "        print(corpus_tf_f[i])\n",
    "        print(corpus_tfidf_f[i])\n",
    "        print(corpus_lsi_f[i])\n",
    "        print(corpus_lda_f[i])\n",
    "        break\n",
    "        #\"\"\"\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rs = 42 # reproducible results, set to None for random\n",
    "(corpus_tf_f_train, corpus_tf_f_test, corpus_tfidf_f_train, corpus_tfidf_f_test, \n",
    " corpus_lsi_f_train, corpus_lsi_f_test, corpus_lda_f_train, corpus_lda_f_test,\n",
    " labels_f_train, labels_f_test) = train_test_split(\n",
    "    corpus_tf_f, corpus_tfidf_f, corpus_lsi_f, corpus_lda_f, labels_f,\n",
    "    test_size=0.2, stratify=labels_f, random_state=rs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dummy_classifier = DummyClassifier(strategy=\"most_frequent\", random_state=rs)\n",
    "dummy_classifier.fit(corpus_tf_f_train, labels_f_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_pred = dummy_classifier.predict(corpus_tf_f_test)\n",
    "print(classification_report(labels_f_test, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#parameters = {'C': [10e-10, 0.1, 1, 10, 10e10]}\n",
    "#lr = GridSearchCV(LogisticRegression(class_weight='balanced', solver='lbfgs'), \n",
    "#                  parameters, cv=1)\n",
    "lr = LogisticRegression(class_weight='balanced', tol=0.1, solver='sag')\n",
    "lr.fit(corpus_tf_f_train, labels_f_train)\n",
    "#print(lr.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_pred = lr.predict(corpus_tf_f_test)\n",
    "print(classification_report(labels_f_test, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lr.fit(corpus_tfidf_f_train, labels_f_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_pred = lr.predict(corpus_tfidf_f_test)\n",
    "print(classification_report(labels_f_test, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lr.fit(corpus_lsi_f_train, labels_f_train)\n",
    "y_pred = lr.predict(corpus_lsi_f_test)\n",
    "print(classification_report(labels_f_test, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lr.fit(corpus_lda_f_train, labels_f_train)\n",
    "y_pred = lr.predict(corpus_lda_f_test)\n",
    "print(classification_report(labels_f_test, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
