{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Tutorial - Taxi pickup problem\n",
    "\n",
    "Welcome to the practical component of this Reinforcement Learning (RL) tutorial! \n",
    "\n",
    "In order to experiment with different RL algorithms and understand how they work, their characteristics, and important challanges in RL, we have prepared a relatively simple toy problem. The idea was to have a problem that is easy to understand and relate to, but at the same time is computatinally efficient (we cannot wait 1h for an algorithm to run in the context of a tutorial) and one where learned policies can be easily visualized. \n",
    "\n",
    "<img src=\"http://mlsm.man.dtu.dk/env.gif\"/>\n",
    "\n",
    "**Problem:** The problem that we will consider is the Taxi pickup problem, where to goal is to pickup customers in a simple 5x5 grid world in order to maximize profit. Real-world extensions of this toy problem could be, for example: \n",
    "\n",
    "- Dynamic routing of a fleet of autonomous vehicles (e.g. each vehicle needs to decide where to roam and which routes to take)\n",
    "\n",
    "- Rebalancing in shared mobility services (e.g. SHARENOW or Green Mobility)\n",
    "\n",
    "- Managing a fleet of autonomous vehicles in a warehouse\n",
    "\n",
    "- Etc.\n",
    "\n",
    "**Environments:** We will consider 3 environments of increasing complexity:\n",
    "\n",
    "1. An environment where there is only a single request at a fixed location, and all the taxi needs to learn to do is to go to that location and pickup the customer. The episode terminates after.\n",
    "\n",
    "2. An environment with 3 different pickup locations. The customers spawn at these locations with different rates. Every time a customer is picked up the taxi is teleported to a random dropout location and continues from there.\n",
    "\n",
    "3. An environment where requests can appear anywhere in the map at any time.\n",
    "\n",
    "**Actions:** For all environments, at each time step, the taxi is allowed to take one of the following 5 actions:\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east \n",
    "- 3: move west \n",
    "- 4: pickup passenger\n",
    "\n",
    "Before we move on, let's make sure that you have installed all the necessary Python packages. Uncomment the following lines of code and run them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras-rl2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following is necessary for Colab:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://mlsm.man.dtu.dk/taxi_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Next we are going to import the Python packages that we will be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Embedding, Reshape\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "import gym\n",
    "\n",
    "from taxi_env import TaxiPickupEnvSimplified, TaxiPickupEnvStandard, TaxiPickupEnvAdvanced\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Simplified environment (Q-learning)\n",
    "\n",
    "In this first part, we will consider a very simple environment where the goal is to drive the taxi to the target location indicated in the grid world by a red 'T'. The taxi is represented by a yellow rectangle. The 5x5 grid world has a few maze-like walls that prevent the taxi to take certain actions when in some positions (i.e. the action has no effect). The taxi has no intermediate rewards. Only one final big reward when it accomplishes the goal (reach the target and take action \"pickup\"). Note also that the taxi has no information about the environment besides its current position (row and column). It has no idea how the environment works and what leads to high rewards!\n",
    "\n",
    "Let's formalize the MDP for this problem:\n",
    "\n",
    "**Actions:** north, south, east, west, pickup\n",
    "\n",
    "**State:** position of the taxi (5x5=25 possible states)\n",
    "\n",
    "**Reward:** +20 if taxi at target location (\"T\") and action is \"pickup\", else -1 (penalty for time elapsed); trying to pickup in a location different than the target also leads to penalty of -10.\n",
    "\n",
    "Implementation details: we have implemented the environment as a class extension of the popular OpenAI Gym. The ``TaxiPickupEnvSimplified`` class therefore extends the ``discrete.Env`` of OpenAI Gym, and provides the following methods:\n",
    "\n",
    "- ``state = env.reset()`` - resets the environment to a random initial state and returns that state;\n",
    "\n",
    "- ``new_state, reward, done, info = env.step(action)`` - takes the ``action`` passed in the argument (integer) and returns the new state, the reward obtained, a boolean ``done`` indicating whether the episode has terminated, and some extra ``info`` (not relevant to this environment);\n",
    "\n",
    "- ``env.render()`` - visualizes the current state of the environment;\n",
    "\n",
    "- ``env.close()`` - terminates the environment.\n",
    "\n",
    "Suggestion: have a look at the ``taxi_env.py`` file to see how the environment dynamics in the ``step()`` function of the ``TaxiPickupEnvSimplified`` class are implemented.\n",
    "\n",
    "### Run Random Policy\n",
    "\n",
    "Let us start by exploring the environment and the functionality described above using a random policy (i.e. taking random actions in the environment). Analyze the code below and run it (note that you can stop execution at any time). Make sure that everything makes sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| : | : : |\n",
      "| : | : :\u001b[43m \u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[31mT\u001b[0m| : | : |\n",
      "+---------+\n",
      "T: 15; Total earnings: 0\n",
      "Action: South; Reward: -1\n"
     ]
    }
   ],
   "source": [
    "env = TaxiPickupEnvSimplified()\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    try:\n",
    "        env.render()\n",
    "        env.step(env.action_space.sample()) # take a random action\n",
    "        time.sleep(1)\n",
    "        clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check properties of the environment ``env`` such as the action space and the observation/state space using the code below. You can check other properties available by writting \"env.\" and then pressing TAB in a cell, although they are not necessary for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Discrete(5)\n",
      "State Space Discrete(25)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Q-learning\n",
    "\n",
    "You will now implement a Q-learning algorithm for learning an implicit policy that tries to optimize rewards in the environment described above. \n",
    "\n",
    "Since the states are discrete, we can represent the Q-function using a table with 25 rows (because there are 5x5 possible states) and 5 columns (because there are 5 possible actions). Each cell then contains the expected future rewards when the agent takes a certain action when in a given state. The Q-learning algorithm can then be summarized as follows:\n",
    "\n",
    "1. initialize Q-table $\\hat{Q}_\\phi$ (e.g. with zeros);\n",
    "2. set learning rate $\\alpha$, discount factor $\\gamma$ and initial exploration parameter $\\epsilon$;\n",
    "3. repeat until maximum number of episodes has been reached:\n",
    "    1. reset environment;\n",
    "    2. repeat until episode ends or maximum number of steps is reached:\n",
    "        1. choose action $a_i$ from $\\epsilon$-greedy policy: with probability $\\epsilon$ select a random action $a_i$, otherwise select $a_i = \\arg \\max_{a_i} \\hat{Q}_\\phi(s_i,a_i)$;\n",
    "        2. take action $a_i$ in the environment and observe new state $s'_i$ and reward $r(s_i,a_i)$;\n",
    "        3. compute target values $y_i \\leftarrow r(s_i,a_i) + \\gamma \\max_{a'_i} \\hat{Q}_\\phi(s'_i,a'_i)$;\n",
    "        4. update Q-table: $\\hat{Q}_\\phi(s_i,a_i) \\leftarrow (1-\\alpha) \\hat{Q}_\\phi(s_i,a_i) + \\alpha y_i $;\n",
    "    3. decay exploration parameter $\\epsilon$;\n",
    "    \n",
    "Note that the algorithm described above uses an $\\epsilon$-greedy exploration policy. In practice, one usually decays the value of $\\epsilon$ over time, until a minimum value of $\\epsilon$ is reached. Also, we will keep track of the rewards over time for later analysis and plotting.\n",
    "\n",
    "So, let us begin by making the necessary initializations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the hyperparameters\n",
    "              \n",
    "alpha = 0.6 #learning rate                 \n",
    "discount_factor = 0.98               \n",
    "epsilon = 1                  \n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01         \n",
    "decay = 0.001\n",
    "\n",
    "train_episodes = 5000   \n",
    "# test_episodes = 100          \n",
    "max_episode_len = 200\n",
    "\n",
    "#Initializing the Q-table\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "#Creating lists to keep track of reward and epsilon values\n",
    "training_rewards = []  \n",
    "epsilons = []\n",
    "episode = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, it is time to implement the Q-learning algorithm as described above. Can you do it? (the skeleton of the code is already there)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 200] Avg reward over last 10 episodes: -88.600\n",
      "alpha=0.600; eps=0.821\n",
      "[Episode 400] Avg reward over last 10 episodes: -27.500\n",
      "alpha=0.600; eps=0.674\n",
      "[Episode 600] Avg reward over last 10 episodes: -0.700\n",
      "alpha=0.600; eps=0.554\n",
      "[Episode 800] Avg reward over last 10 episodes: 0.200\n",
      "alpha=0.600; eps=0.455\n",
      "[Episode 1000] Avg reward over last 10 episodes: 3.100\n",
      "alpha=0.600; eps=0.375\n",
      "[Episode 1200] Avg reward over last 10 episodes: 4.600\n",
      "alpha=0.600; eps=0.308\n",
      "[Episode 1400] Avg reward over last 10 episodes: 14.300\n",
      "alpha=0.600; eps=0.254\n",
      "[Episode 1600] Avg reward over last 10 episodes: 10.400\n",
      "alpha=0.600; eps=0.210\n",
      "[Episode 1800] Avg reward over last 10 episodes: 11.600\n",
      "alpha=0.600; eps=0.174\n",
      "[Episode 2000] Avg reward over last 10 episodes: 12.000\n",
      "alpha=0.600; eps=0.144\n",
      "[Episode 2200] Avg reward over last 10 episodes: 14.000\n",
      "alpha=0.600; eps=0.120\n",
      "[Episode 2400] Avg reward over last 10 episodes: 13.300\n",
      "alpha=0.600; eps=0.100\n",
      "[Episode 2600] Avg reward over last 10 episodes: 14.800\n",
      "alpha=0.600; eps=0.084\n",
      "[Episode 2800] Avg reward over last 10 episodes: 14.400\n",
      "alpha=0.600; eps=0.070\n",
      "[Episode 3000] Avg reward over last 10 episodes: 12.900\n",
      "alpha=0.600; eps=0.059\n",
      "[Episode 3200] Avg reward over last 10 episodes: 13.700\n",
      "alpha=0.600; eps=0.050\n",
      "[Episode 3400] Avg reward over last 10 episodes: 14.100\n",
      "alpha=0.600; eps=0.043\n",
      "[Episode 3600] Avg reward over last 10 episodes: 14.100\n",
      "alpha=0.600; eps=0.037\n",
      "[Episode 3800] Avg reward over last 10 episodes: 15.200\n",
      "alpha=0.600; eps=0.032\n",
      "[Episode 4000] Avg reward over last 10 episodes: 13.800\n",
      "alpha=0.600; eps=0.028\n",
      "[Episode 4200] Avg reward over last 10 episodes: 15.400\n",
      "alpha=0.600; eps=0.025\n",
      "[Episode 4400] Avg reward over last 10 episodes: 16.100\n",
      "alpha=0.600; eps=0.022\n",
      "[Episode 4600] Avg reward over last 10 episodes: 14.000\n",
      "alpha=0.600; eps=0.020\n",
      "[Episode 4800] Avg reward over last 10 episodes: 14.600\n",
      "alpha=0.600; eps=0.018\n",
      "[Episode 5000] Avg reward over last 10 episodes: 15.300\n",
      "alpha=0.600; eps=0.017\n",
      "Training score over time: -2.3712\n"
     ]
    }
   ],
   "source": [
    "for _ in range(train_episodes):\n",
    "    episode += 1\n",
    "    if episode % 200 == 0 and episode != 0:\n",
    "        print(\"[Episode %d] Avg reward over last 10 episodes: %.3f\" % (episode, np.mean(training_rewards[-10:])))\n",
    "        print(\"alpha=%.3f; eps=%.3f\" % (alpha, epsilon))\n",
    "        \n",
    "    # Reseting the environment each time as per requirement\n",
    "    state = env.reset()    \n",
    "    \n",
    "    # Starting the tracker for the rewards\n",
    "    total_training_rewards = 0\n",
    "    for step in range(max_episode_len):\n",
    "        \n",
    "        # Choosing an action given the states based on a random number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1) \n",
    "        \n",
    "        ### SECOND option for choosing the initial action - exploit     \n",
    "        # If the random number is larger than epsilon: employing exploitation \n",
    "        # and selecting best action \n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(Q[state,:]) \n",
    "        \n",
    "        ### FIRST option for choosing the initial action - explore       \n",
    "        # Otherwise, employing exploration: choosing a random action \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        ### performing the action and getting the reward     \n",
    "        # Taking the action and getting the reward and outcome state\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        ### update the Q-table\n",
    "        # Updating the Q-table using the Bellman equation\n",
    "        Q[state, action] = Q[state, action]+alpha*(reward+discount_factor*np.max(Q[new_state, :])-Q[state, action]) \n",
    "        \n",
    "        # Increasing our total reward and updating the state\n",
    "        total_training_rewards += reward      \n",
    "        state = new_state         \n",
    "        \n",
    "        # Ending the episode\n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "    # Cutting down on exploration by reducing the epsilon \n",
    "    epsilon = min_epsilon+(max_epsilon-min_epsilon)*np.exp(-decay*episode)\n",
    "    \n",
    "    # Adding the total reward and reduced epsilon values\n",
    "    training_rewards.append(total_training_rewards)\n",
    "    epsilons.append(epsilon)\n",
    "    \n",
    "print (\"Training score over time: \" + str(sum(training_rewards)/train_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the rewards improve?\n",
    "\n",
    "Let's plot the evolutions of episodic rewards over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_rewards);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also the values of $\\epsilon$ (useful for debugging purposes and tweaking the decay rate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epsilons);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the (implicit) policy learned in the Q-table to see if it corresponds to good behaviour in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| : | : : |\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[43m\u001b[31mT\u001b[0m\u001b[0m| : | : |\n",
      "+---------+\n",
      "T: 6; Total earnings: 0\n",
      "Action: South; Reward: -1\n"
     ]
    }
   ],
   "source": [
    "env = TaxiPickupEnvSimplified()\n",
    "state = env.reset()\n",
    "for _ in range(1000):\n",
    "    try:\n",
    "        env.render()\n",
    "        action = np.argmax(Q[state,:])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        time.sleep(1)\n",
    "        clear_output(wait=True)\n",
    "        if done == True:\n",
    "            env.render()\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is the policy learned? Does it do what you expected it to do?\n",
    "\n",
    "We can further inspect the Q-values for different actions for when the taxi is in a certain position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values for different actions a when taxi in location (1, 2)\n",
      "Action a=South\t Q[s,a]=899.1\n",
      "Action a=North\t Q[s,a]=861.5\n",
      "Action a=East\t Q[s,a]=861.5\n",
      "Action a=West\t Q[s,a]=880.1\n",
      "Action a=Pickup\t Q[s,a]=871.1\n"
     ]
    }
   ],
   "source": [
    "taxi_loc = (1,2)\n",
    "print(\"Q-values for different actions a when taxi in location\", taxi_loc)\n",
    "for a in range(env.nA):\n",
    "    print(\"Action a=%s\\t Q[s,a]=%.1f\" % (env.action_names[a], Q[env.encode(taxi_loc),a]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, the maximum Q-value should correspond to the action that takes the taxi towards the target the most from that position.\n",
    "\n",
    "Lastly, we can derive the value function (value table, in this case) that correspond to that Q-table. Remember, the value function tells us the value of being in a certain state (i.e. expected future rewards averaged over all possible actions according to the current policy $\\pi$):\n",
    "\n",
    "\\begin{align}\n",
    "V^\\pi(s) = \\mathbb{E}_{a \\sim \\pi(a|s)}[ Q^\\pi(s,a) ]  \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "Since in Q-learning, the agent always chooses the action that maximizes $Q^\\pi(s_t,a_t)$ (well, unless during training where it uses $\\epsilon$-greedy to explore), the value function is simply given by:\n",
    "\n",
    "\\begin{align}\n",
    "V^\\pi(s) = \\max_a  Q^\\pi(s,a)   \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "Let's compute the value table and visualize it by color-coding it according to its values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "V = np.zeros((env.num_rows, env.num_columns))\n",
    "for s in range(env.nS):\n",
    "    taxi_row, taxi_col = env.decode(s)\n",
    "    V[taxi_row, taxi_col] = max([Q[s,a] for a in range(env.nA)]) # max Q-value for all possible actions\n",
    "\n",
    "plt.imshow(V)\n",
    "plt.title(\"Value function V(s)\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the result make sense to you? The closer the taxi is to the objective, the higher the value of that state should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Standard environment (Q-learning)\n",
    "\n",
    "In this part, we will consider a more interesting version of the taxi pickup problem. There will now be 3 possible pickups locations (represented in Red, Green and Blue) where customers arrive with different rates. The number of customers currently waiting at a given pickups location is shown as a red, green and blue counters in the map. The goal is for the taxi to pickup customers and maximize reward. To simplify the environment implementation, when a customer is picked up, the taxi teleports to the destination and continues from there. \n",
    "\n",
    "With these changes, we now have a more complex environment and therefore we need a more complex representation to encode its state. There are still 5x5=25 possible locations for the taxi to be, but now, there can also be $2^3=8$ types of situations: \n",
    "\n",
    "- no customers waiting in any location\n",
    "\n",
    "- customers waiting in red location, and no customers waiting elsewhere\n",
    "\n",
    "- customers waiting in green location, and no customers waiting elsewhere \n",
    "\n",
    "- customers waiting in blue location, and no customers waiting elsewhere \n",
    "\n",
    "- customers waiting in red and green locations, and no customers waiting in blue location\n",
    "\n",
    "- customers waiting in red and blue locations, and no customers waiting in green location\n",
    "\n",
    "- customers waiting in green and blue locations, and no customers waiting in red location\n",
    "\n",
    "- customers waiting in all 3 locations\n",
    "\n",
    "Note that we do not differentiate between having 1 or 2 or N customers waiting at a location to simplify things.\n",
    "\n",
    "Therefore, the state representation of the environment now has $5\\times5\\times2^3 = 200$ dimensions, and we need to estimate Q-values for all possible actions $a \\in \\mathcal{A}$ for all those 200 possible states! That is starting to look like a challanging task, right? And this is still a tiny 5x5 grid world... (more on scalability later :-)\n",
    "\n",
    "In summary, the new MDP for this problem can be formalized as:\n",
    "\n",
    "**Actions:** north, south, east, west, pickup\n",
    "\n",
    "**State:** position of the taxi and status of requests at 3 different locations: $5\\times5\\times2^3 = 200$ dimensions\n",
    "\n",
    "**Reward:** +20 if taxi at a pickup location where there are requests and action is \"pickup\", else -1 (penalty for time elapsed); trying to pickup in a location different than the target also leads to penalty of -10.\n",
    "\n",
    "### Run Random Policy\n",
    "\n",
    "We start again by trying out the environmenet using a random policy (note that you can stop execution at any time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| :\u001b[31m0\u001b[0m| : : |\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[32m0\u001b[0m| : |\u001b[34m0\u001b[0m: |\n",
      "+---------+\n",
      "T: 5; Total earnings: 0\n",
      "Action: South; Reward: -1\n"
     ]
    }
   ],
   "source": [
    "env = TaxiPickupEnvStandard()\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    try:\n",
    "        env.render()\n",
    "        env.step(env.action_space.sample()) # take a random action\n",
    "        time.sleep(1)\n",
    "        clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Discrete(5)\n",
      "State Space Discrete(200)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Q-learning\n",
    "\n",
    "We will now apply the same Q-learning algorithm that you've implemented above for the simplified environment. Can you take care of the missing code blocks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the hyperparameters\n",
    "              \n",
    "alpha = 0.6 #learning rate                 \n",
    "discount_factor = 0.98               \n",
    "epsilon = 1                  \n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01         \n",
    "decay = 0.001\n",
    "\n",
    "train_episodes = 10000           \n",
    "max_episode_len = 200\n",
    "\n",
    "#Initializing the Q-table\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "#Creating lists to keep track of reward and epsilon values\n",
    "training_rewards = []  \n",
    "epsilons = []\n",
    "episode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 200] Avg reward over last 10 episodes: -356.600\n",
      "alpha=0.600; eps=0.821\n",
      "[Episode 400] Avg reward over last 10 episodes: -176.600\n",
      "alpha=0.600; eps=0.674\n",
      "[Episode 600] Avg reward over last 10 episodes: -91.100\n",
      "alpha=0.600; eps=0.554\n",
      "[Episode 800] Avg reward over last 10 episodes: 71.200\n",
      "alpha=0.600; eps=0.455\n",
      "[Episode 1000] Avg reward over last 10 episodes: 98.500\n",
      "alpha=0.600; eps=0.375\n",
      "[Episode 1200] Avg reward over last 10 episodes: 163.900\n",
      "alpha=0.600; eps=0.308\n",
      "[Episode 1400] Avg reward over last 10 episodes: 205.000\n",
      "alpha=0.600; eps=0.254\n",
      "[Episode 1600] Avg reward over last 10 episodes: 230.800\n",
      "alpha=0.600; eps=0.210\n",
      "[Episode 1800] Avg reward over last 10 episodes: 206.500\n",
      "alpha=0.600; eps=0.174\n",
      "[Episode 2000] Avg reward over last 10 episodes: 305.800\n",
      "alpha=0.600; eps=0.144\n",
      "[Episode 2200] Avg reward over last 10 episodes: 282.100\n",
      "alpha=0.600; eps=0.120\n",
      "[Episode 2400] Avg reward over last 10 episodes: 339.100\n",
      "alpha=0.600; eps=0.100\n",
      "[Episode 2600] Avg reward over last 10 episodes: 342.400\n",
      "alpha=0.600; eps=0.084\n",
      "[Episode 2800] Avg reward over last 10 episodes: 297.700\n",
      "alpha=0.600; eps=0.070\n",
      "[Episode 3000] Avg reward over last 10 episodes: 319.600\n",
      "alpha=0.600; eps=0.059\n",
      "[Episode 3200] Avg reward over last 10 episodes: 325.900\n",
      "alpha=0.600; eps=0.050\n",
      "[Episode 3400] Avg reward over last 10 episodes: 355.300\n",
      "alpha=0.600; eps=0.043\n",
      "[Episode 3600] Avg reward over last 10 episodes: 376.900\n",
      "alpha=0.600; eps=0.037\n",
      "[Episode 3800] Avg reward over last 10 episodes: 356.500\n",
      "alpha=0.600; eps=0.032\n",
      "[Episode 4000] Avg reward over last 10 episodes: 441.100\n",
      "alpha=0.600; eps=0.028\n",
      "[Episode 4200] Avg reward over last 10 episodes: 338.200\n",
      "alpha=0.600; eps=0.025\n",
      "[Episode 4400] Avg reward over last 10 episodes: 377.500\n",
      "alpha=0.600; eps=0.022\n",
      "[Episode 4600] Avg reward over last 10 episodes: 316.000\n",
      "alpha=0.600; eps=0.020\n",
      "[Episode 4800] Avg reward over last 10 episodes: 384.100\n",
      "alpha=0.600; eps=0.018\n",
      "[Episode 5000] Avg reward over last 10 episodes: 359.200\n",
      "alpha=0.600; eps=0.017\n",
      "[Episode 5200] Avg reward over last 10 episodes: 390.100\n",
      "alpha=0.600; eps=0.015\n",
      "[Episode 5400] Avg reward over last 10 episodes: 306.700\n",
      "alpha=0.600; eps=0.014\n",
      "[Episode 5600] Avg reward over last 10 episodes: 392.500\n",
      "alpha=0.600; eps=0.014\n",
      "[Episode 5800] Avg reward over last 10 episodes: 394.900\n",
      "alpha=0.600; eps=0.013\n",
      "[Episode 6000] Avg reward over last 10 episodes: 418.900\n",
      "alpha=0.600; eps=0.012\n",
      "[Episode 6200] Avg reward over last 10 episodes: 295.600\n",
      "alpha=0.600; eps=0.012\n",
      "[Episode 6400] Avg reward over last 10 episodes: 363.100\n",
      "alpha=0.600; eps=0.012\n",
      "[Episode 6600] Avg reward over last 10 episodes: 394.900\n",
      "alpha=0.600; eps=0.011\n",
      "[Episode 6800] Avg reward over last 10 episodes: 307.000\n",
      "alpha=0.600; eps=0.011\n",
      "[Episode 7000] Avg reward over last 10 episodes: 339.100\n",
      "alpha=0.600; eps=0.011\n",
      "[Episode 7200] Avg reward over last 10 episodes: 363.100\n",
      "alpha=0.600; eps=0.011\n",
      "[Episode 7400] Avg reward over last 10 episodes: 384.400\n",
      "alpha=0.600; eps=0.011\n",
      "[Episode 7600] Avg reward over last 10 episodes: 400.900\n",
      "alpha=0.600; eps=0.010\n",
      "[Episode 7800] Avg reward over last 10 episodes: 431.800\n",
      "alpha=0.600; eps=0.010\n",
      "[Episode 8000] Avg reward over last 10 episodes: 379.600\n",
      "alpha=0.600; eps=0.010\n",
      "[Episode 8200] Avg reward over last 10 episodes: 338.800\n",
      "alpha=0.600; eps=0.010\n",
      "[Episode 8400] Avg reward over last 10 episodes: 368.500\n",
      "alpha=0.600; eps=0.010\n",
      "[Episode 8600] Avg reward over last 10 episodes: 364.600\n",
      "alpha=0.600; eps=0.010\n",
      "[Episode 8800] Avg reward over last 10 episodes: 466.300\n",
      "alpha=0.600; eps=0.010\n",
      "[Episode 9000] Avg reward over last 10 episodes: 391.900\n",
      "alpha=0.600; eps=0.010\n",
      "[Episode 9200] Avg reward over last 10 episodes: 441.100\n",
      "alpha=0.600; eps=0.010\n",
      "[Episode 9400] Avg reward over last 10 episodes: 373.000\n",
      "alpha=0.600; eps=0.010\n",
      "[Episode 9600] Avg reward over last 10 episodes: 394.900\n",
      "alpha=0.600; eps=0.010\n",
      "[Episode 9800] Avg reward over last 10 episodes: 341.500\n",
      "alpha=0.600; eps=0.010\n",
      "[Episode 10000] Avg reward over last 10 episodes: 406.600\n",
      "alpha=0.600; eps=0.010\n",
      "Training score over time: 292.0597\n"
     ]
    }
   ],
   "source": [
    "for _ in range(train_episodes):\n",
    "    episode += 1\n",
    "    if episode % 200 == 0 and episode != 0:\n",
    "        print(\"[Episode %d] Avg reward over last 10 episodes: %.3f\" % (episode, np.mean(training_rewards[-10:])))\n",
    "        print(\"alpha=%.3f; eps=%.3f\" % (alpha, epsilon))\n",
    "        \n",
    "    # Reseting the environment each time as per requirement\n",
    "    state = env.reset()    \n",
    "    \n",
    "    # Starting the tracker for the rewards\n",
    "    total_training_rewards = 0\n",
    "    for step in range(max_episode_len):\n",
    "        \n",
    "        # Choosing an action given the states based on a random number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1) \n",
    "        \n",
    "        ### SECOND option for choosing the initial action - exploit     \n",
    "        # If the random number is larger than epsilon: employing exploitation \n",
    "        # and selecting best action \n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(Q[state,:]) \n",
    "        \n",
    "        ### FIRST option for choosing the initial action - explore       \n",
    "        # Otherwise, employing exploration: choosing a random action \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        ### performing the action and getting the reward     \n",
    "        # Taking the action and getting the reward and outcome state\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        ### update the Q-table\n",
    "        # Updating the Q-table using the Bellman equation\n",
    "        Q[state, action] = Q[state, action]+alpha*(reward+discount_factor*np.max(Q[new_state, :])-Q[state, action]) \n",
    "        \n",
    "        # Increasing our total reward and updating the state\n",
    "        total_training_rewards += reward      \n",
    "        state = new_state         \n",
    "        \n",
    "        # Ending the episode\n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "    # Cutting down on exploration by reducing the epsilon \n",
    "    epsilon = min_epsilon+(max_epsilon-min_epsilon)*np.exp(-decay*episode)\n",
    "    \n",
    "    # Adding the total reward and reduced epsilon values\n",
    "    training_rewards.append(total_training_rewards)\n",
    "    epsilons.append(epsilon)\n",
    "    \n",
    "print (\"Training score over time: \" + str(sum(training_rewards)/train_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the results? Let's have a look at the rewards over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_rewards);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also the values of $\\epsilon$ (useful for debugging purposes and tweaking the decay rate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epsilons);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the behaviour of the learned policy in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| :\u001b[31m0\u001b[0m| : : |\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[43m\u001b[32m3\u001b[0m\u001b[0m| : |\u001b[34m0\u001b[0m: |\n",
      "+---------+\n",
      "T: 41; Total earnings: 120\n",
      "Action: South; Reward: -1\n"
     ]
    }
   ],
   "source": [
    "env = TaxiPickupEnvStandard()\n",
    "state = env.reset()\n",
    "for _ in range(1000):\n",
    "    try:\n",
    "        env.render()\n",
    "        action = np.argmax(Q[state,:])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        time.sleep(1)\n",
    "        clear_output(wait=True)\n",
    "        if done == True:\n",
    "            env.render()\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you managed to learn a policy that maybe does not perform perfectly, but does a pretty good job at maximizing rewards. \n",
    "\n",
    "Now try editing the ``TaxiPickupEnvStandard`` class in the file ``taxi_env.py`` and change either the request rate of one of the locations to be (much) higher than the others (i.e. simulating that one location is more popular than the others), or change the earnings of one of the locations such that pickups in that location lead to (much) higher rewards (e.g. simulating a \"fancy\" part of town where customers give very generous tips). Re-run the Q-learning algorithm and notice how the learned policy changes and how new behaviours emerge :-)\n",
    "\n",
    "Hint: Look for the following lines in the ``taxi_env.py`` file:\n",
    "\n",
    "``self.req_rates = req_rates = [5e-2, 5e-2, 5e-2]``\n",
    "\n",
    "``if pickup_loc == 0: \n",
    "    reward = 20 \n",
    "elif pickup_loc == 1:\n",
    "    reward = 20 \n",
    "elif pickup_loc == 2:\n",
    "    reward = 20 ``\n",
    "\n",
    "**Important: You will have to reload the TaxiPickupEnvStandard class in order for changes in taxi_env.py to take effect. Use the following code to reload. If that doesn't work, then restart the Python kernel from the Jupyter menus on the top.**\n",
    "\n",
    "``import importlib\n",
    "importlib.reload(taxi_env)``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Advanced environment (Deep Q-learning)\n",
    "\n",
    "In this part, we will consider a more complex environment where pickup requests can appear anywhere on our 5x5 grid world. When there are requests in a map cell, we will represent that with a red counter showing the number of requests in that cell.\n",
    "\n",
    "### Run Random Policy\n",
    "\n",
    "As we have been doing before, let's start by having a look at how that environment looks like using a random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "| | : | : |\n",
      "+---------+\n",
      "T: 3; Total earnings: 0\n",
      "Action: West; Reward: -1\n"
     ]
    }
   ],
   "source": [
    "env = TaxiPickupEnvAdvanced()\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    try:\n",
    "        env.render()\n",
    "        state = env.step(env.action_space.sample()) # take a random action\n",
    "        time.sleep(1)\n",
    "        clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Discrete(5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space {}\".format(env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-learning (DQN)\n",
    "\n",
    "As you probably realized already, when we start considering more interesting and more realistic problems, the dimensionality of the state space can quickly become unmanegeable for standard tabular Q-learning approaches. In the environment from Part 2, with just 3 possible pickup locations, the number of possible (discrete) states was already $5\\times5\\times2^3 = 200$. This is not a problem just because of memory requirements, but the learning task of fitting Q-table becomes much more complex (observations becomes sparser - curse of dimensionality)! Not only that, but what happens when the states are continuous rather than discrete? We definitely need a way of tackling these problems...\n",
    "\n",
    "This is where function approximation comes in! In this tutorial, we will use neural networks as function approximators. This is also by far the most popular approach in the recent RL literature. To makes things easier, we will rely on 2 Python packages - keras and keras-rl - that abstract away many of the complexities involved in creating a neural network and using it for deep Q-learning.\n",
    "\n",
    "**Optional: Implementing neural networks in Keras** (if you want, you may skip this part and treat the neural network as black-box function approximator)\n",
    "\n",
    "Bulding a multi-layer neural network in Keras is fairly easy. We start by creating an object of the class \"Sequential\" (indicating that the neural network consists of sequence of layers):\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "```\n",
    "\n",
    "Now we can add layers to our neural network model. For examply, we can add a fully connected (dense) layer with 50 neurons and using a ReLU (rectified linear unit) activation function as follows:\n",
    "\n",
    "```python\n",
    "model.add(Dense(50, input_dim=30, activation='relu'))\n",
    "```\n",
    "\n",
    "A similar approach can be used for other types of layers such as Convolutional Layers. The following line adds a Convolutional layer with 20 filters of 3x3 convolutions:\n",
    "\n",
    "```python\n",
    "model.add(Conv2D(20, kernel_size=(3, 3), activation=\"relu\"))\n",
    "```\n",
    "We can now keep adding more hidden layers, or add the final Dense layer. Note that since this is a regression problem (i.e. we want the neural network to output Q-value for the different possible actions given the state passed as input to it), the last layer (output layer) of network must necessarily have as many neurons/outputs as there are actions in our RL problem and it must use a linear activation:\n",
    "\n",
    "```python\n",
    "model.add(Dense(env.nA, activation='linear'))\n",
    "```\n",
    "\n",
    "**MDP formulation**:\n",
    "\n",
    "Armed with the power of neural networks for function approximation, we can build a more complex representation of the environment state. In this case, we will represent the state using 4 5x5 matrices containing:\n",
    "\n",
    "- Matrix 1: position of the taxi, one-hot encoded (i.e. with \"1\" in the place where the taxi is located, and zeros else where). For example:\n",
    "\n",
    "``[[0,0,0,0,0],\n",
    " [0,0,0,0,0],\n",
    " [0,0,0,0,0],\n",
    " [0,0,0,1,0],\n",
    " [0,0,0,0,0]]``\n",
    "\n",
    "- Matrix 2: number of pickup requests in each cell. For example:\n",
    "\n",
    "``[[0,0,0,0,0],\n",
    " [0,0,0,1,0],\n",
    " [0,0,0,0,0],\n",
    " [2,0,0,0,4],\n",
    " [0,0,1,0,0]]``\n",
    " \n",
    "- Matrix 3: information about the presence of walls to the east. In this case:\n",
    "\n",
    "``[[0,1,0,0,0],\n",
    " [0,1,0,0,0],\n",
    " [0,0,0,0,0],\n",
    " [1,0,1,0,0],\n",
    " [1,0,1,0,0]]``\n",
    " \n",
    "- Matrix 4: information about the presence of walls to the west. In this case:\n",
    "\n",
    "``[[0,0,1,0,0],\n",
    " [0,0,1,0,0],\n",
    " [0,0,0,0,0],\n",
    " [0,1,0,1,0],\n",
    " [0,1,0,1,0]]``\n",
    " \n",
    "We can stack these 4 matrices together to create a 5x5x4 tensor that represents the state of the environment, and feed it to the neural network as input. The idea is for the neural network to learn to approximate the Q-values (i.e. expected future rewards) for the different possible actions given the state that was passed as input.\n",
    "\n",
    "Aside: we are not arguing that this is necessarily the best state representation for this problem. In fact, coming up with good state representation is a key challenge in deep RL and it can determine how fast your agent learns and how the policies that it learns can be. This state representation seems to perform acceptably well for this problem, so we will proceed with it.\n",
    "\n",
    "In summary, the new MDP for this revised version of the problem can be formalized as:\n",
    "\n",
    "**Actions:** north, south, east, west, pickup\n",
    "\n",
    "**State:** 5x5x4 tensor representing the position of the taxi, the number of requests in each cell and the locations of the \"walls\"\n",
    "\n",
    "**Reward:** +20 if taxi at a pickup location where there are requests and action is \"pickup\", else -1 (penalty for time elapsed); trying to pickup in a location different than the target also leads to penalty of -10.\n",
    "\n",
    "The code below creates a neural network in Keras that performs well in this environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 5, 5, 4)           0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 3, 3, 20)          740       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 905       \n",
      "=================================================================\n",
      "Total params: 1,645\n",
      "Trainable params: 1,645\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Embedding, Reshape, Conv2D, MaxPooling2D, Input\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# First, we build a very simple neural network model in Keras\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(1, env.num_rows, env.num_columns, 4)))\n",
    "model.add(Reshape(target_shape=(env.num_rows, env.num_columns, 4)))\n",
    "model.add(Conv2D(20, kernel_size=(3, 3), activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(env.nA, activation='linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the neural network in place, it is time to use it as a function approximator for the Q-function of our RL agent. This can be easily done using the Keras-rl package in Python. The code below creates a deep Q-learning agent using $\\epsilon$-greedy exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# Then, define DQN agent in Keras-RL\n",
    "memory = SequentialMemory(limit=200000, window_length=1)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                              attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=100000)\n",
    "dqn = DQNAgent(model=model, nb_actions=env.nA, memory=memory, policy=policy, \n",
    "               nb_steps_warmup=500, target_model_update=1e-2, enable_double_dqn=True, enable_dueling_network=True)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, we are doing the exact some thing as we did before: Q-learning with $\\epsilon$-greedy exploration. The key difference is in the way that we approximate the Q-function: before we used a table, and now we are using a neural network. \n",
    "\n",
    "As you can probably guess from the code above, we are also using a few popular RL techniques that improve the stability and convergence of Q-learning algorithms:\n",
    "\n",
    "- Experience replay: we add a ``memory`` that allows the RL agent to \"re-live\" past experience but accounting for the latest knowledge that it has about the Q-function.\n",
    "\n",
    "- Double deep Q networks (``enable_double_dqn=True``)\n",
    "\n",
    "- Dueling networks (``enable_dueling_network=True``)\n",
    "\n",
    "These fall outside of the scope of this tutorial, but they are explained in detail in the aditional materials provided in the last slides.\n",
    "\n",
    "We can now run our deep Q-learning algorithm (in this case for 400000 steps, where each episode has a maximum of 200 steps):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 400000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "WARNING:tensorflow:From /home/rodr/env36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: -1.4167\n",
      "50 episodes - episode_reward: -283.340 [-488.000, 124.000] - loss: 13.864 - mae: 14.810 - mean_q: 13.563 - mean_eps: 0.953\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: -1.1464\n",
      "50 episodes - episode_reward: -229.280 [-482.000, -53.000] - loss: 61.906 - mae: 100.863 - mean_q: 129.805 - mean_eps: 0.865\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 85s 8ms/step - reward: -0.7825\n",
      "50 episodes - episode_reward: -156.500 [-356.000, 52.000] - loss: 120.319 - mae: 157.243 - mean_q: 200.484 - mean_eps: 0.775\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 87s 9ms/step - reward: -0.8218\n",
      "50 episodes - episode_reward: -164.360 [-431.000, 79.000] - loss: 150.138 - mae: 172.814 - mean_q: 220.014 - mean_eps: 0.685\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 85s 8ms/step - reward: -0.4246\n",
      "50 episodes - episode_reward: -84.920 [-266.000, 178.000] - loss: 147.101 - mae: 170.613 - mean_q: 217.219 - mean_eps: 0.595\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: -0.2839\n",
      "50 episodes - episode_reward: -56.780 [-380.000, 139.000] - loss: 138.279 - mae: 166.424 - mean_q: 211.918 - mean_eps: 0.505\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 85s 9ms/step - reward: -0.0844\n",
      "50 episodes - episode_reward: -16.880 [-305.000, 289.000] - loss: 131.326 - mae: 160.701 - mean_q: 204.763 - mean_eps: 0.415\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 89s 9ms/step - reward: -0.0808\n",
      "50 episodes - episode_reward: -16.160 [-203.000, 322.000] - loss: 124.536 - mae: 155.065 - mean_q: 197.676 - mean_eps: 0.325\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 91s 9ms/step - reward: 0.4091\n",
      "50 episodes - episode_reward: 81.820 [-122.000, 340.000] - loss: 120.329 - mae: 149.466 - mean_q: 190.719 - mean_eps: 0.235\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 100s 10ms/step - reward: 0.5168\n",
      "50 episodes - episode_reward: 103.360 [-212.000, 301.000] - loss: 118.402 - mae: 146.347 - mean_q: 186.832 - mean_eps: 0.145\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 103s 10ms/step - reward: 0.6410\n",
      "50 episodes - episode_reward: 128.200 [-101.000, 322.000] - loss: 111.443 - mae: 143.151 - mean_q: 182.838 - mean_eps: 0.100\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 106s 11ms/step - reward: 0.6218\n",
      "50 episodes - episode_reward: 124.360 [-281.000, 433.000] - loss: 112.999 - mae: 140.996 - mean_q: 180.107 - mean_eps: 0.100\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 113s 11ms/step - reward: 0.6725\n",
      "50 episodes - episode_reward: 134.500 [-110.000, 445.000] - loss: 112.871 - mae: 140.308 - mean_q: 179.259 - mean_eps: 0.100\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 121s 12ms/step - reward: 0.7004\n",
      "50 episodes - episode_reward: 140.080 [-149.000, 310.000] - loss: 103.204 - mae: 138.896 - mean_q: 177.507 - mean_eps: 0.100\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 127s 13ms/step - reward: 0.7316\n",
      "50 episodes - episode_reward: 146.320 [-125.000, 442.000] - loss: 107.858 - mae: 138.842 - mean_q: 177.452 - mean_eps: 0.100\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 130s 13ms/step - reward: 0.8297\n",
      "50 episodes - episode_reward: 165.940 [-218.000, 463.000] - loss: 108.652 - mae: 137.403 - mean_q: 175.693 - mean_eps: 0.100\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 134s 13ms/step - reward: 0.6950\n",
      "50 episodes - episode_reward: 139.000 [-65.000, 418.000] - loss: 103.314 - mae: 138.933 - mean_q: 177.631 - mean_eps: 0.100\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 137s 14ms/step - reward: 0.8165\n",
      "50 episodes - episode_reward: 163.300 [-101.000, 454.000] - loss: 109.281 - mae: 137.635 - mean_q: 175.972 - mean_eps: 0.100\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 141s 14ms/step - reward: 0.8834\n",
      "50 episodes - episode_reward: 176.680 [-119.000, 451.000] - loss: 104.310 - mae: 136.172 - mean_q: 174.136 - mean_eps: 0.100\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 146s 15ms/step - reward: 1.0703\n",
      "50 episodes - episode_reward: 214.060 [-71.000, 568.000] - loss: 104.492 - mae: 138.203 - mean_q: 176.699 - mean_eps: 0.100\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 146s 15ms/step - reward: 1.0397\n",
      "50 episodes - episode_reward: 207.940 [-320.000, 529.000] - loss: 103.676 - mae: 137.530 - mean_q: 175.820 - mean_eps: 0.100\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 150s 15ms/step - reward: 1.0115\n",
      "50 episodes - episode_reward: 202.300 [-20.000, 430.000] - loss: 98.044 - mae: 135.766 - mean_q: 173.634 - mean_eps: 0.100\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 147s 15ms/step - reward: 1.1036\n",
      "50 episodes - episode_reward: 220.720 [-158.000, 442.000] - loss: 95.857 - mae: 132.720 - mean_q: 169.901 - mean_eps: 0.100\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 1.0448\n",
      "50 episodes - episode_reward: 208.960 [-47.000, 487.000] - loss: 95.440 - mae: 134.750 - mean_q: 172.646 - mean_eps: 0.100\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 147s 15ms/step - reward: 1.1735\n",
      "50 episodes - episode_reward: 234.700 [-23.000, 526.000] - loss: 96.831 - mae: 134.193 - mean_q: 171.912 - mean_eps: 0.100\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 1.2107\n",
      "50 episodes - episode_reward: 242.140 [85.000, 493.000] - loss: 94.963 - mae: 134.702 - mean_q: 172.558 - mean_eps: 0.100\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 150s 15ms/step - reward: 1.0400\n",
      "50 episodes - episode_reward: 208.000 [-191.000, 598.000] - loss: 94.262 - mae: 131.226 - mean_q: 168.279 - mean_eps: 0.100\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 147s 15ms/step - reward: 1.0481\n",
      "50 episodes - episode_reward: 209.620 [-29.000, 466.000] - loss: 91.323 - mae: 130.995 - mean_q: 168.052 - mean_eps: 0.100\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 147s 15ms/step - reward: 1.0829\n",
      "50 episodes - episode_reward: 216.580 [10.000, 619.000] - loss: 90.271 - mae: 130.014 - mean_q: 166.781 - mean_eps: 0.100\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.9863\n",
      "50 episodes - episode_reward: 197.260 [16.000, 409.000] - loss: 90.761 - mae: 129.471 - mean_q: 166.080 - mean_eps: 0.100\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 150s 15ms/step - reward: 0.9986\n",
      "50 episodes - episode_reward: 199.720 [-494.000, 475.000] - loss: 86.779 - mae: 130.439 - mean_q: 167.338 - mean_eps: 0.100\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 1.1933\n",
      "50 episodes - episode_reward: 238.660 [-104.000, 583.000] - loss: 90.127 - mae: 129.817 - mean_q: 166.561 - mean_eps: 0.100\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 149s 15ms/step - reward: 1.1966\n",
      "50 episodes - episode_reward: 239.320 [-47.000, 442.000] - loss: 90.131 - mae: 128.967 - mean_q: 165.490 - mean_eps: 0.100\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 150s 15ms/step - reward: 1.1570\n",
      "50 episodes - episode_reward: 231.400 [46.000, 523.000] - loss: 88.005 - mae: 127.427 - mean_q: 163.484 - mean_eps: 0.100\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 147s 15ms/step - reward: 1.1498\n",
      "50 episodes - episode_reward: 229.960 [1.000, 475.000] - loss: 84.153 - mae: 125.840 - mean_q: 161.503 - mean_eps: 0.100\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 151s 15ms/step - reward: 0.9935\n",
      "50 episodes - episode_reward: 198.700 [-65.000, 346.000] - loss: 87.436 - mae: 125.071 - mean_q: 160.527 - mean_eps: 0.100\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 150s 15ms/step - reward: 1.1450\n",
      "50 episodes - episode_reward: 229.000 [-23.000, 442.000] - loss: 84.708 - mae: 126.072 - mean_q: 161.753 - mean_eps: 0.100\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 1.1603\n",
      "50 episodes - episode_reward: 232.060 [37.000, 496.000] - loss: 85.924 - mae: 125.394 - mean_q: 160.972 - mean_eps: 0.100\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 151s 15ms/step - reward: 1.1504\n",
      "50 episodes - episode_reward: 230.080 [76.000, 466.000] - loss: 83.842 - mae: 125.963 - mean_q: 161.638 - mean_eps: 0.100\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 151s 15ms/step - reward: 1.2896\n",
      "done, took 5105.105 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2825c83a20>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=400000, visualize=False, verbose=1, nb_max_episode_steps=200, log_interval=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the RL agent is learned, we can visualize the learned policy by exploiting the call-back functionality in Keras (don't worry if this code is a bit confusing for you as a first time Keras user; focus on the results that you obtained instead):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class Visualizer(Callback):\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    \n",
    "    def on_action_end(self, action, logs):\n",
    "        \"\"\" Render environment at the end of each action \"\"\"\n",
    "        self.env.render(mode='human')\n",
    "        time.sleep(1)\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| : | : : |\n",
      "|\u001b[31m2\u001b[0m: | : : |\n",
      "|\u001b[31m1\u001b[0m:\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "| |\u001b[31m1\u001b[0m: | : |\n",
      "+---------+\n",
      "T: 98; Total earnings: 360\n",
      "Action: North; Reward: -1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dqn.test(env, nb_episodes=5, callbacks=[Visualizer(env)], nb_max_episode_steps=99, visualize=False, verbose=0)\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the results? Does the policy do what you would expect it to do? Or does it sometimes behaves strangely (i.e. not perfectly)? When I ran this code I managed to obtain a pretty good policy in a relatively short amount of time, but you can try doing a few tweaks to see if you improve. For example, try:\n",
    "\n",
    "- Increasing the number of training steps ``nb_steps=400000``\n",
    "\n",
    "- Increasing the maximum length of each episode ``nb_max_episode_steps=200``\n",
    "\n",
    "- Increasing the complexity of the neural network (e.g. more layers or more neurons per layer)\n",
    "\n",
    "- Changing the learning rate of the neural network optimizer ``Adam(lr=1e-3)``\n",
    "\n",
    "- Other hyper-parameters of the RL algorithm (e.g. memory size, decay rate of $\\epsilon$-greedy, etc.):\n",
    "\n",
    "``memory = SequentialMemory(limit=200000, window_length=1)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                              attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=100000)\n",
    "dqn = DQNAgent(model=model, nb_actions=env.nA, memory=memory, policy=policy, \n",
    "               nb_steps_warmup=500, target_model_update=1e-2, enable_double_dqn=True, enable_dueling_network=True)``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We hope that you enjoyed this brief introduction to the world of reinforcement learning :-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
